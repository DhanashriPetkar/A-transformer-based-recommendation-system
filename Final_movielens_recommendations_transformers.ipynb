{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhanashriPetkar/A-transformer-based-recommendation-system/blob/main/Final_movielens_recommendations_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8Zi3AIGeXJ"
      },
      "source": [
        "# A Transformer-based recommendation system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlZrz83EGeXQ"
      },
      "source": [
        "## Introduction\n",
        "The Behavior Sequence Transformer (BST) model predicts movie ratings using the Movielens dataset. It uses sequences of movies a user has watched, the ratings they've given, and user and movie features.\n",
        "\n",
        "Inputs to the model are:\n",
        "\n",
        "1. Sequence of watched movie IDs.\n",
        "2. Sequence of ratings for these movies.\n",
        "3. User features: ID, sex, occupation, age group.\n",
        "4. Genres for each movie and the target movie.\n",
        "5. Target movie ID to predict the rating.\n",
        "\n",
        "Changes in this example:\n",
        "\n",
        "1. Incorporate movie genres directly into the movie embeddings.\n",
        "2. Use movie ratings and their positions to update sequences before self-attention.\n",
        "\n",
        "Requires TensorFlow 2.4+."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asO0CSqhGeXS"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We use the Movielens 1M dataset, which has about 1 million ratings from 6000 users on 4000 movies. It includes user features, movie genres, and timestamps for each rating. The timestamps help create sequences of movie ratings for each user, needed for the BST model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXlHAVN_GeXT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7HvRPTBKGeXU"
      },
      "outputs": [],
      "source": [
        "# Import the os module to interact with the operating system\n",
        "import os\n",
        "\n",
        "# Set the Keras backend to TensorFlow\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "# Import the math module for mathematical functions\n",
        "import math\n",
        "\n",
        "# Import the ZipFile class from the zipfile module to handle zip files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Import the urlretrieve function from urllib.request to download files from a URL\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Import the keras module for building neural networks\n",
        "import keras\n",
        "\n",
        "# Import numpy for numerical operations on arrays\n",
        "import numpy as np\n",
        "\n",
        "# Import pandas for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Import tensorflow for machine learning and deep learning tasks\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import layers from keras for building neural network layers\n",
        "from keras import layers\n",
        "\n",
        "# Import StringLookup from keras.layers for converting strings to integer indices\n",
        "from keras.layers import StringLookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQNLQidmGeXW"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "### Download and prepare the DataFrames\n",
        "\n",
        "First, download the Movielens data.\n",
        "\n",
        "The folder will have three files: `users.dat`,` movies.dat`, and `ratings.dat`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bXhTfRMgGeXW"
      },
      "outputs": [],
      "source": [
        "# Download the MovieLens dataset zip file from the specified URL and save it as \"movielens.zip\"\n",
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
        "\n",
        "# Extract all the contents of the \"movielens.zip\" file into the current directory\n",
        "ZipFile(\"movielens.zip\", \"r\").extractall()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLA_JUTzGeXX"
      },
      "source": [
        "Then, we load the data into pandas DataFrames with their proper column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zRV6aCOCGeXY"
      },
      "outputs": [],
      "source": [
        "# Read the users data from the \"users.dat\" file into a pandas DataFrame\n",
        "users = pd.read_csv(\n",
        "    \"ml-1m/users.dat\",\n",
        "    sep=\"::\",  # Specify the separator used in the file\n",
        "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],  # Define column names\n",
        "    encoding=\"ISO-8859-1\",  # Specify the encoding of the file\n",
        "    engine=\"python\",  # Specify the parsing engine\n",
        ")\n",
        "\n",
        "# Read the ratings data from the \"ratings.dat\" file into a pandas DataFrame\n",
        "ratings = pd.read_csv(\n",
        "    \"ml-1m/ratings.dat\",\n",
        "    sep=\"::\",  # Specify the separator used in the file\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],  # Define column names\n",
        "    encoding=\"ISO-8859-1\",  # Specify the encoding of the file\n",
        "    engine=\"python\",  # Specify the parsing engine\n",
        ")\n",
        "\n",
        "# Read the movies data from the \"movies.dat\" file into a pandas DataFrame\n",
        "movies = pd.read_csv(\n",
        "    \"ml-1m/movies.dat\",\n",
        "    sep=\"::\",  # Specify the separator used in the file\n",
        "    names=[\"movie_id\", \"title\", \"genres\"],  # Define column names\n",
        "    encoding=\"ISO-8859-1\",  # Specify the encoding of the file\n",
        "    engine=\"python\",  # Specify the parsing engine\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izWKgAnWGeXY"
      },
      "source": [
        "Here, we do some simple data processing to fix the data types of the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ai3suXTsGeXZ"
      },
      "outputs": [],
      "source": [
        "# Update the user_id column in the users DataFrame to prepend \"user_\" to each user_id\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "\n",
        "# Update the age_group column in the users DataFrame to prepend \"group_\" to each age_group\n",
        "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
        "\n",
        "# Update the occupation column in the users DataFrame to prepend \"occupation_\" to each occupation\n",
        "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
        "\n",
        "# Update the movie_id column in the movies DataFrame to prepend \"movie_\" to each movie_id\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "# Update the movie_id column in the ratings DataFrame to prepend \"movie_\" to each movie_id\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "# Update the user_id column in the ratings DataFrame to prepend \"user_\" to each user_id\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "\n",
        "# Convert the rating column in the ratings DataFrame to float\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB8_kupVGeXZ"
      },
      "source": [
        "Each movie has multiple genres. We split them into separate columns in the `movies`\n",
        "DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eD6CNfoTGeXZ"
      },
      "outputs": [],
      "source": [
        "# List of genres\n",
        "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
        "          \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
        "          \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
        "\n",
        "# Extend the genres list with genres from the \"genres\" column in the movies DataFrame\n",
        "for genre in genres:\n",
        "    # Apply a lambda function to create binary indicators for each genre\n",
        "    movies[genre] = movies[\"genres\"].apply(\n",
        "        lambda values: int(genre in values.split(\"|\"))  # Convert genre presence into 0 or 1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGgD82EKGeXZ"
      },
      "source": [
        "### Transform the movie ratings data into sequences\n",
        "\n",
        "First, sort the ratings data by `unix_timestamp`. Then, group `movie_id` and rating by `user_id`.\n",
        "\n",
        "The output DataFrame will have each `user_id` with two ordered lists: the movies they rated and their `ratings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PnOSNH2-GeXa"
      },
      "outputs": [],
      "source": [
        "# Sort the ratings DataFrame by \"unix_timestamp\" and then group by \"user_id\"\n",
        "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
        "\n",
        "# Create a new DataFrame ratings_data from the grouped data\n",
        "ratings_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),  # List of user_ids\n",
        "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),  # List of lists of movie_ids per user\n",
        "        \"ratings\": list(ratings_group.rating.apply(list)),  # List of lists of ratings per user\n",
        "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),  # List of lists of timestamps per user\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K9q4AXzGeXa"
      },
      "source": [
        "Split the `movie_ids` and `ratings` lists into sequences of a fixed length. Adjust the `sequence_length` variable to change the input length, and `step_size` to control the number of sequences per user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "F7hgety_GeXa"
      },
      "outputs": [],
      "source": [
        "# Define the sequence length and step size\n",
        "sequence_length = 4\n",
        "step_size = 2\n",
        "\n",
        "# Function to create sequences of a specified window size and step size\n",
        "def create_sequences(values, window_size, step_size):\n",
        "    sequences = []\n",
        "    start_index = 0\n",
        "    while True:\n",
        "        end_index = start_index + window_size\n",
        "        seq = values[start_index:end_index]\n",
        "\n",
        "        # If the sequence is shorter than the window size, pad with the last window\n",
        "        if len(seq) < window_size:\n",
        "            seq = values[-window_size:]\n",
        "            if len(seq) == window_size:\n",
        "                sequences.append(seq)\n",
        "            break\n",
        "\n",
        "        # Append the sequence to the list of sequences\n",
        "        sequences.append(seq)\n",
        "\n",
        "        # Move the start index forward by the step size\n",
        "        start_index += step_size\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Apply create_sequences to the movie_ids column in ratings_data\n",
        "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "# Apply create_sequences to the ratings column in ratings_data\n",
        "ratings_data.ratings = ratings_data.ratings.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "# Remove the \"timestamps\" column from ratings_data\n",
        "del ratings_data[\"timestamps\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glMouoRoGeXa"
      },
      "source": [
        "After that, we process the output to have each sequence in a separate records in\n",
        "the DataFrame. In addition, we join the user features with the ratings data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LD6qIiu-GeXb"
      },
      "outputs": [],
      "source": [
        "# Explode the \"movie_ids\" column in ratings_data and create a new DataFrame ratings_data_movies\n",
        "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
        "    \"movie_ids\", ignore_index=True\n",
        ")\n",
        "\n",
        "# Explode the \"ratings\" column in ratings_data and create a new DataFrame ratings_data_rating\n",
        "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
        "\n",
        "# Concatenate ratings_data_movies and ratings_data_rating along axis 1 to combine them\n",
        "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
        "\n",
        "# Join the users DataFrame with ratings_data_transformed on \"user_id\"\n",
        "ratings_data_transformed = ratings_data_transformed.join(\n",
        "    users.set_index(\"user_id\"), on=\"user_id\"\n",
        ")\n",
        "\n",
        "# Convert the \"movie_ids\" column to a comma-separated string of movie IDs\n",
        "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
        "    lambda x: \",\".join(x)\n",
        ")\n",
        "\n",
        "# Convert the \"ratings\" column to a comma-separated string of ratings\n",
        "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
        "    lambda x: \",\".join([str(v) for v in x])\n",
        ")\n",
        "\n",
        "# Remove the \"zip_code\" column from ratings_data_transformed\n",
        "del ratings_data_transformed[\"zip_code\"]\n",
        "\n",
        "# Rename the columns \"movie_ids\" to \"sequence_movie_ids\" and \"ratings\" to \"sequence_ratings\"\n",
        "ratings_data_transformed.rename(\n",
        "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVwjBGtBGeXb"
      },
      "source": [
        "With `sequence_length` of 4 and `step_size` of 2, we end up with 498,623 sequences.\n",
        "\n",
        "Finally, we split the data into training and testing splits, with 85% and 15% of\n",
        "the instances, respectively, and store them to CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xl0Zj-rqGeXc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a boolean mask for random selection of rows\n",
        "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
        "\n",
        "# Select rows for the train_data DataFrame based on the random_selection mask\n",
        "train_data = ratings_data_transformed[random_selection]\n",
        "\n",
        "# Select rows for the test_data DataFrame based on the inverse of random_selection mask\n",
        "test_data = ratings_data_transformed[~random_selection]\n",
        "\n",
        "# Save train_data to a CSV file without index, using \"|\" as separator and without header\n",
        "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
        "\n",
        "# Save test_data to a CSV file without index, using \"|\" as separator and without header\n",
        "test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NQYQ__cGeXc"
      },
      "source": [
        "## Define metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "V_QHGpFzGeXd"
      },
      "outputs": [],
      "source": [
        "# Define CSV_HEADER as a list of column names from ratings_data_transformed\n",
        "CSV_HEADER = list(ratings_data_transformed.columns)\n",
        "\n",
        "# Define CATEGORICAL_FEATURES_WITH_VOCABULARY as a dictionary of categorical features and their unique values\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"user_id\": list(users.user_id.unique()),        # List of unique user IDs\n",
        "    \"movie_id\": list(movies.movie_id.unique()),     # List of unique movie IDs\n",
        "    \"sex\": list(users.sex.unique()),                # List of unique values from the 'sex' column in users\n",
        "    \"age_group\": list(users.age_group.unique()),    # List of unique age groups from the 'age_group' column in users\n",
        "    \"occupation\": list(users.occupation.unique()),  # List of unique occupations from the 'occupation' column in users\n",
        "}\n",
        "\n",
        "# Define USER_FEATURES as a list of categorical features related to users\n",
        "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
        "\n",
        "# Define MOVIE_FEATURES as a list of categorical features related to movies\n",
        "MOVIE_FEATURES = [\"genres\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAOhFypmGeXd"
      },
      "source": [
        "## Create `tf.data.Dataset` for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AUdKUfM0GeXd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the function get_dataset_from_csv to create a TensorFlow dataset from a CSV file\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    # Define the process function to process features and target from CSV data\n",
        "    def process(features):\n",
        "        # Split sequence_movie_ids string into a tensor\n",
        "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
        "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
        "\n",
        "        # The last movie id in the sequence is the target movie.\n",
        "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
        "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
        "\n",
        "        # Split sequence_ratings string into a tensor and convert to float32\n",
        "        ratings_string = features[\"sequence_ratings\"]\n",
        "        sequence_ratings = tf.strings.to_number(\n",
        "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
        "        ).to_tensor()\n",
        "\n",
        "        # The last rating in the sequence is the target for the model to predict.\n",
        "        target = sequence_ratings[:, -1]\n",
        "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
        "\n",
        "        return features, target\n",
        "\n",
        "    # Create a TensorFlow dataset from the CSV file using make_csv_dataset\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        field_delim=\"|\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(process)  # Apply the process function to each batch in the dataset\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDo9oOGfGeXd"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vSBqtP-pGeXe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define a function create_model_inputs to create input layers for the model\n",
        "def create_model_inputs():\n",
        "    return {\n",
        "        # User-related inputs\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
        "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
        "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
        "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
        "\n",
        "        # Sequence inputs\n",
        "        \"sequence_movie_ids\": keras.Input(\n",
        "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"target_movie_id\": keras.Input(\n",
        "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"sequence_ratings\": keras.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vgsq8idGeXe"
      },
      "source": [
        "## Encode input features\n",
        "\n",
        "The `encode_input_features` method works as follows:\n",
        "\n",
        "1. Each categorical user feature is encoded using `layers.Embedding`, with embedding\n",
        "dimension equals to the square root of the vocabulary size of the feature.\n",
        "The embeddings of these features are concatenated to form a single input tensor.\n",
        "\n",
        "2. Each movie in the movie sequence and the target movie is encoded `layers.Embedding`,\n",
        "where the dimension size is the square root of the number of movies.\n",
        "\n",
        "3. A multi-hot genres vector for each movie is concatenated with its embedding vector,\n",
        "and processed using a non-linear `layers.Dense` to output a vector of the same movie\n",
        "embedding dimensions.\n",
        "\n",
        "4. A positional embedding is added to each movie embedding in the sequence, and then\n",
        "multiplied by its rating from the ratings sequence.\n",
        "\n",
        "5. The target movie embedding is concatenated to the sequence movie embeddings, producing\n",
        "a tensor with the shape of `[batch size, sequence length, embedding size]`, as expected\n",
        "by the attention layer for the transformer architecture.\n",
        "\n",
        "6. The method returns a tuple of two elements:  `encoded_transformer_features` and\n",
        "`encoded_other_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7ybxifldGeXe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "\n",
        "# Define a function encode_input_features to encode input features for the model\n",
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        movie_idx = movie_index_lookup(movie_id)\n",
        "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
        "        encoded_movie = movie_embedding\n",
        "        if include_movie_features:\n",
        "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
        "            encoded_movie = movie_embedding_processor(\n",
        "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
        "            )\n",
        "        return encoded_movie\n",
        "\n",
        "    ## Encoding target_movie_id\n",
        "    target_movie_id = inputs[\"target_movie_id\"]\n",
        "    encoded_target_movie = encode_movie(target_movie_id)\n",
        "\n",
        "    ## Encoding sequence_movie_ids\n",
        "    sequence_movie_ids = inputs[\"sequence_movie_ids\"]\n",
        "    encoded_sequence_movies = encode_movie(sequence_movie_ids)\n",
        "\n",
        "    ## Create positional embedding\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encoded_positions = position_embedding_encoder(positions)\n",
        "\n",
        "    ## Retrieve sequence ratings to incorporate them into the movie encoding\n",
        "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
        "    sequence_ratings = tf.expand_dims(sequence_ratings, -1)\n",
        "\n",
        "    ## Add positional encoding to movie encodings and multiply them by rating\n",
        "    encoded_sequence_movies_with_position_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_movies + encoded_positions), sequence_ratings]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs\n",
        "    for i in range(sequence_length - 1):\n",
        "        feature = encoded_sequence_movies_with_position_and_rating[:, i, ...]\n",
        "        feature = tf.expand_dims(feature, 1)\n",
        "        encoded_transformer_features.append(feature)\n",
        "    encoded_transformer_features.append(encoded_target_movie)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSNq3u9GGeXe"
      },
      "source": [
        "## Create a BST model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vi57A_nNGeXe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "# Define global variables\n",
        "include_user_id = False\n",
        "include_user_features = False\n",
        "include_movie_features = False\n",
        "\n",
        "hidden_units = [256, 128]\n",
        "dropout_rate = 0.1\n",
        "num_heads = 3\n",
        "\n",
        "# Function to create model inputs\n",
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
        "        \"sequence_movie_ids\": keras.Input(\n",
        "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"target_movie_id\": keras.Input(\n",
        "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"sequence_ratings\": keras.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
        "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
        "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
        "    }\n",
        "\n",
        "# Function to encode input features\n",
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        movie_idx = movie_index_lookup(movie_id)\n",
        "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
        "        encoded_movie = movie_embedding\n",
        "        if include_movie_features:\n",
        "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
        "            encoded_movie = movie_embedding_processor(\n",
        "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
        "            )\n",
        "        return encoded_movie\n",
        "\n",
        "    ## Encoding target_movie_id\n",
        "    target_movie_id = inputs[\"target_movie_id\"]\n",
        "    encoded_target_movie = encode_movie(target_movie_id)\n",
        "\n",
        "    ## Encoding sequence_movie_ids\n",
        "    sequence_movie_ids = inputs[\"sequence_movie_ids\"]\n",
        "    encoded_sequence_movies = encode_movie(sequence_movie_ids)\n",
        "\n",
        "    ## Create positional embedding\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encoded_positions = position_embedding_encoder(positions)\n",
        "\n",
        "    ## Retrieve sequence ratings to incorporate them into the movie encoding\n",
        "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
        "    sequence_ratings = tf.expand_dims(sequence_ratings, -1)\n",
        "\n",
        "    ## Add positional encoding to movie encodings and multiply them by rating\n",
        "    encoded_sequence_movies_with_position_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_movies + encoded_positions), sequence_ratings]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs\n",
        "    for i in range(sequence_length - 1):\n",
        "        feature = encoded_sequence_movies_with_position_and_rating[:, i, ...]\n",
        "        feature = tf.expand_dims(feature, 1)\n",
        "        encoded_transformer_features.append(feature)\n",
        "    encoded_transformer_features.append(encoded_target_movie)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features\n",
        "\n",
        "# Function to create the model\n",
        "def create_model():\n",
        "    inputs = create_model_inputs()\n",
        "    transformer_features, other_features = encode_input_features(\n",
        "        inputs, include_user_id, include_user_features, include_movie_features\n",
        "    )\n",
        "\n",
        "    # Create a multi-headed attention layer\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
        "    )(transformer_features, transformer_features)\n",
        "\n",
        "    # Transformer block\n",
        "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
        "    x1 = layers.Add()([transformer_features, attention_output])\n",
        "    x1 = layers.LayerNormalization()(x1)\n",
        "    x2 = layers.LeakyReLU()(x1)\n",
        "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
        "    x2 = layers.Dropout(dropout_rate)(x2)\n",
        "    transformer_features = layers.Add()([x1, x2])\n",
        "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
        "    features = layers.Flatten()(transformer_features)\n",
        "\n",
        "    # Include the other features\n",
        "    if other_features is not None:\n",
        "        features = layers.concatenate(\n",
        "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
        "        )\n",
        "\n",
        "    # Fully-connected layers\n",
        "    for num_units in hidden_units:\n",
        "        features = layers.Dense(num_units)(features)\n",
        "        features = layers.BatchNormalization()(features)\n",
        "        features = layers.LeakyReLU()(features)\n",
        "        features = layers.Dropout(dropout_rate)(features)\n",
        "\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwp2OboLGeXf"
      },
      "source": [
        "## Run training and evaluation experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-8Okr2MXGeXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f911ece3-2f09-46fc-e721-e46da5fa0fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1601/1601 [==============================] - 34s 19ms/step - loss: 1.2324 - mean_absolute_error: 0.8827\n",
            "Epoch 2/5\n",
            "1601/1601 [==============================] - 28s 17ms/step - loss: 1.0100 - mean_absolute_error: 0.8014\n",
            "Epoch 3/5\n",
            "1601/1601 [==============================] - 27s 17ms/step - loss: 0.9551 - mean_absolute_error: 0.7787\n",
            "Epoch 4/5\n",
            "1601/1601 [==============================] - 28s 17ms/step - loss: 0.9279 - mean_absolute_error: 0.7675\n",
            "Epoch 5/5\n",
            "1601/1601 [==============================] - 26s 16ms/step - loss: 0.9080 - mean_absolute_error: 0.7590\n",
            "Test MAE: 0.78\n"
          ]
        }
      ],
      "source": [
        "# Compile the model.\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),  # Use Adagrad optimizer with learning rate 0.01\n",
        "    loss=keras.losses.MeanSquaredError(),  # Mean Squared Error loss function\n",
        "    metrics=[keras.metrics.MeanAbsoluteError()],  # Mean Absolute Error as the evaluation metric\n",
        ")\n",
        "\n",
        "# Read the training data.\n",
        "train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n",
        "\n",
        "# Fit the model with the training data.\n",
        "model.fit(train_dataset, epochs=5)  # Train the model for 5 epochs\n",
        "\n",
        "# Read the test data.\n",
        "test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n",
        "\n",
        "# Evaluate the model on the test data.\n",
        "_, mae = model.evaluate(test_dataset, verbose=0)  # Evaluate and get Mean Absolute Error\n",
        "print(f\"Test MAE: {round(mae, 3)}\")  # Print the Mean Absolute Error rounded to 3 decimal places"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q2Q3HZwGeXf"
      },
      "source": [
        "You should achieve a Mean Absolute Error (MAE) at or around 0.7 on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3DICmxGeXf"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The BST model uses the Transformer layer in its architecture to capture the sequential signals underlying\n",
        "usersâ€™ behavior sequences for recommendation.\n",
        "\n",
        "You can try training this model with different configurations, for example, by increasing\n",
        "the input sequence length and training the model for a larger number of epochs. In addition,\n",
        "you can try including other features like movie release year and customer\n",
        "zipcode, and including cross features like sex X genre."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}